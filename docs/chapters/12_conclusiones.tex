\chapter{Conclusiones}

Estudiar un ámbito tan denso como es el aprendizaje por refuerzo (o la inteligencia artificial en general) puede llegar a ser bastante abrumador, especialmente cuando hay tanta variedad de recursos accesibles a través de Internet, en los que cada libro o artículo propone distintas definiciones o notaciones para un mismo concepto, haciendo que el contraste de información sea confuso. Por este motivo, el enfoque que se le ha dado a este trabajo de fin de grado ha sido que pueda servir como referencia para aquellas personas interesadas en el aprendizaje por refuerzo.

Para ello, en el trascurso de este proyecto, se ha hecho un recorrido sobre distintos métodos de aprendizaje por refuerzo sin modelos, recogiendo de manera coherente los conceptos teóricos que han ido apareciendo y relacionándolos entre sí. Concretamente, se ha hecho un estudio extenso sobre los métodos basados en el valor y en la política, mostrando algunos ejemplos de algoritmos como Q-learning o REINFORCE para una mejor comprensión, para finalmente entender cómo funcionan los métodos actor-crítico.

Finalmente, para poner en práctica los conocimientos adquiridos y resolver el problema planteado inicialmente, se ha implementado en Unity un entorno de aprendizaje basado en el pádel, donde se han integrado agentes proporcionados por el Unity ML-Agents Toolkit. Estos agentes se definieron en el contexto de aprendizaje por refuerzo, especificando sus espacios de observaciones, acciones y funciones de recompensa, y se entrenaron mediante algoritmos como Proximal Policy Optimization o Curiosity, para dotarles un comportamiento humano y simular las decisiones óptimas que haría un jugador profesional de pádel.

En general, los resultados obtenidos han sido positivos, por lo que se considera que se ha cumplido satisfactoriamente con los objetivos propuestos inicialmente. Aun así, debido a los contratiempos surgidos durante el desarrollo, como consecuencia de la aparición de errores en la implementación, o la inviabilidad de ciertas funcionalidades, algunas pruebas como el uso del aprendizaje por imitación no se pudieron llevar a cabo. Por esta razón, se plantean a continuación algunas propuestas de trabajo futuro.

\section{Trabajo futuro}

Aunque los resultados obtenidos en este proyecto hayan sido satisfactorios, al tratarse precisamente de una primera versión, la cual ha servido sobre todo como introducción al aprendizaje por refuerzo, hay aún mucho margen de mejora en relación al diseño del entorno de aprendizaje. Algunas propuestas de mejora de cara al futuro son:
\begin{enumerate}
    \item[-] Refinamiento de la función de recompensa: un problema con las recompensas que se han definido en este entorno es que son muy sencillas. Los valores de estas recompensas, las cuales son constantes durante todo el entrenamiento, podrían variar en función del tiempo trascurrido, por ejemplo, para incentivar a los jugadores a terminar un punto lo antes posible. 
    \newpage
    \item[-] Aplicar un margen de error en la trayectoria de la pelota: sería interesante que se añadiera en el entorno virtual un margen de error en la trayectoria de la pelota. Por ejemplo, la trayectoria se podría desviar en función de parámetros como la velocidad de la pelota o un tiempo de reacción del agente. Así, durante el entrenamiento, los agentes deberían aprender a adoptar estrategias que traten de minimizar este error, aprendiendo, por ejemplo, a dejar botar primero la pelota. Una precisión perfecta es poco realista, ya que en realidad hay muchos factores que influyen en la ejecución de un golpeo.
    \item[-] Reimplementación del proceso de grabación de demostraciones: la aplicación de técnicas de aprendizaje por imitación puede resultar muy útil para facilitar el entrenamiento de agentes. El módulo utilizado para grabar demostraciones se podría mejorar reduciendo la latencia en la comunicación. En particular, se podría probar de implementar todo el proceso en el mismo entorno de Unity, haciendo que la obtención de datos sea mucho más rápida.
    \item[-] Experimentación con otros espacios de observaciones y/o acciones: en el entorno de aprendizaje que se ha implementado, el tamaño del espacio de observaciones y acciones se podría aumentar, planteando un problema aún más complejo. Por ejemplo, se podrían añadir más tipos de golpeo, como la contrapared o las dejadas, y aumentar las dimensiones de la cuadrícula de posiciones. En cuanto al espacio de observaciones, sería interesante añadir otras variables que permitan al agente asociar algunas recompensas a ciertas observaciones en concreto. Por ejemplo, se podrían añadir las posiciones de interés de devolución para que los agentes puedan hacer una mejor predicción sobre la trayectoria de la pelota.
    \item[-] Rediseño de la interfaz de depuración: aunque la interfaz de depuración es funcional, al mostrar todos los datos como un conjunto, a la larga se complicaría la depuración de nuevas variables. Por lo tanto, se propone dividir la interfaz de depuración en diversos menús, manipulables por el usuario, para que el seguimiento las variables sea más cómodo.
\end{enumerate}