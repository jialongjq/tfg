\begin{abstract}
\thispagestyle{plain}
En videojuegos o simuladores, una de las tareas más laboriosas suele ser la atribución de inteligencia a personajes no jugables, especialmente cuando se trata de entornos con un grado elevado de complejidad. En este trabajo de fin de grado se exploran alternativas a la programación tradicional para simular el comportamiento humano en entidades virtuales. En particular, se estudian diversas técnicas de aprendizaje por refuerzo y por imitación para entrenar agentes que, con un conocimiento básico de un entorno de pádel, aprendan a tomar decisiones óptimas en cuanto a posición en la pista, desplazamientos, y acción técnica a realizar.

A lo largo de este proyecto, se han estudiado conceptos clave de los métodos de aprendizaje por refuerzo sin modelo, desde métodos basados en el valor como Q-learning, hasta métodos basados en la política como REINFORCE, con la finalidad de entender la base teórica que hay detrás de algunos algoritmos del estado del arte. Estos algoritmos se han utilizado para entrenar agentes en un entorno de aprendizaje personalizado basado en el pádel implementado en Unity, utilizando el Unity ML-Agents Toolkit. Durante los experimentos, se obtuvieron resultados muy interesantes tras el entrenamiento de agentes mediante el algoritmo de Proximal Policy Optimization, en los que los agentes eran capaces de tomar decisiones muy naturales en diversos escenarios.

\end{abstract}

\begin{otherlanguage}{catalan}
\begin{abstract}
\thispagestyle{plain}
A videojocs o simuladors, una de les tasques més laborioses sol ser l'atribució d'intel·ligència a personatges no jugables, especialment quan es tracta d'entorns amb un grau elevat de complexitat. En aquest treball de fi de grau s'exploren alternatives a la programació tradicional per simular el comportament humà en aquests personatges. En particular, s'estudien diverses tècniques d'aprenentatge per reforç i per imitació per entrenar agents que, amb un coneixement bàsic d'un entorn de pàdel, aprenguin a prendre decisions òptimes pel que fa a la posició a la pista, els desplaçaments i l'acció tècnica a realitzar.

Al llarg d'aquest projecte, s'han estudiat conceptes clau dels mètodes d'aprenentatge per reforç sense model, des de mètodes basats en el valor com el Q-learning, fins a mètodes basats en la política com el REINFORCE, amb la finalitat d'entendre la base teòrica que hi ha darrere d'alguns algorismes de l'estat de l'art. Aquests algoritmes s'han fet servir per entrenar agents en un entorn d'aprenentatge personalitzat basat en el pàdel implementat a Unity, utilitzant l'Unity ML-Agents Toolkit. Durant els experiments, es van obtenir resultats molt interessants després de l'entrenament d'agents mitjançant l'algorisme de Proximal Policy Optimization, en els que els agents eren capaços de prendre decisions molt naturals a diversos escenaris.

\end{abstract}
\end{otherlanguage}

\begin{otherlanguage}{english}
\begin{abstract}
In video games or simulators, one of the most laborious tasks is usually the attribution of intelligence to non-playable characters, especially when dealing with environments with a high degree of complexity. This bachelor's degree final project explores alternatives to traditional programming to simulate human behavior in those characters. In particular, various reinforcement and imitation learning techniques are studied in order to train agents who, with a basic knowledge of the padel environment, will eventually learn to make optimal decisions regarding position on the court, movements, and technical action to be performed.

Throughout this project, many key concepts of model-free reinforced learning methods have been studied, from value-based methods like Q-learning to policy-based methods such as REINFORCE, in order to understand the underlying theory behind some of the state-of-the-art algorithms. These algorithms have then been used to train agents in a customized padel-based learning environment implemented in Unity, using the Unity ML-Agents Toolkit. During the experiments, many positive results were obtained after training agents using Proximal Policy Optimization, where agents were able to make human-like decisions on different scenarios.

\thispagestyle{plain}
\end{abstract}
\end{otherlanguage}